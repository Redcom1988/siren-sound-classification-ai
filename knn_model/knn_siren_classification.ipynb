{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191825be",
   "metadata": {},
   "source": [
    "# **1. Data Handling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7dd897",
   "metadata": {},
   "source": [
    "# 1.1 Load dataset from Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31584bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "dataset_name = 'vishnu0399/emergency-vehicle-siren-sounds'\n",
    "data_path = './data'\n",
    "\n",
    "project_path = os.getcwd()\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = project_path\n",
    "\n",
    "kaggle_json_path = os.path.join(project_path, 'kaggle.json')\n",
    "if not os.path.exists(kaggle_json_path):\n",
    "    print(f\"Warning: kaggle.json not found at {kaggle_json_path}\")\n",
    "    print(\"Please place your kaggle.json file in the project directory\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(f\"Found kaggle.json at {kaggle_json_path}\")\n",
    "\n",
    "api = KaggleApi()\n",
    "\n",
    "try:\n",
    "    api.authenticate()\n",
    "    print(\"Kaggle API authenticated successfully.\")\n",
    "\n",
    "    print(f\"Downloading dataset: {dataset_name} to {data_path}\")\n",
    "    api.dataset_download_files(dataset_name, path=data_path, unzip=True)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Kaggle API authentication failed. Please check your credentials.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902faee5",
   "metadata": {},
   "source": [
    "# 1.2 Dataset file loader and preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f33908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "# === WAV Loader ===\n",
    "def read_wav(filename):\n",
    "    with wave.open(filename, 'rb') as wav_file:\n",
    "        n_channels, sampwidth, framerate, n_frames, _, _ = wav_file.getparams()\n",
    "        raw_data = wav_file.readframes(n_frames)\n",
    "        fmt = \"<\" + \"h\" * (n_frames * n_channels)\n",
    "        data = struct.unpack(fmt, raw_data)\n",
    "        print(f\"üìÇ Membaca file: {filename} (sample rate: {framerate} Hz, total frames: {n_frames})\")\n",
    "        return np.array(data), framerate\n",
    "    \n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0606f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Time Domain ===\n",
    "def zero_crossing_rate(samples):\n",
    "    # Vectorized zero crossing rate calculation\n",
    "    signs = np.sign(samples)\n",
    "    sign_changes = np.diff(signs)\n",
    "    zcr = np.count_nonzero(sign_changes) / len(samples)\n",
    "    print(f\"üîπ Zero Crossing Rate: {zcr:.5f}\")\n",
    "    return zcr\n",
    "\n",
    "def energy(samples):\n",
    "    # Vectorized energy calculation\n",
    "    en = np.mean(samples**2)\n",
    "    print(f\"üîπ Signal Energy: {en:.2f}\")\n",
    "    return en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ad098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "# === Frequency Domain (FFT + Dominant Frequency) ===\n",
    "def fft(samples):\n",
    "    print(f\"üî∏ Applying FTT to {len(samples)} samples...\")\n",
    "    # Use NumPy's FFT implementation\n",
    "    fft_result = np.fft.fft(samples)\n",
    "    magnitudes = np.abs(fft_result)\n",
    "    return magnitudes\n",
    "\n",
    "def dominant_freq(magnitudes, sample_rate):\n",
    "    # Find dominant frequency using NumPy\n",
    "    max_index = np.argmax(magnitudes)\n",
    "    freq = max_index * sample_rate / len(magnitudes)\n",
    "    print(f\"üîπ Dominant Frequency: {freq:.2f} Hz\")\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MFCC ===\n",
    "def pre_emphasis(signal, coeff=0.97):\n",
    "    # Vectorized pre-emphasis filter\n",
    "    emphasized = np.zeros_like(signal)\n",
    "    emphasized[0] = signal[0]\n",
    "    emphasized[1:] = signal[1:] - coeff * signal[:-1]\n",
    "    return emphasized\n",
    "\n",
    "def hamming_window(N):\n",
    "    # Use NumPy's hamming window\n",
    "    return np.hamming(N)\n",
    "\n",
    "def hz_to_mel(hz):\n",
    "    return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "def mel_to_hz(mel):\n",
    "    return 700 * (10**(mel / 2595) - 1)\n",
    "\n",
    "def mel_filterbank(n_filters, N_fft, sample_rate):\n",
    "    # Vectorized mel filterbank creation\n",
    "    low_mel = hz_to_mel(0)\n",
    "    high_mel = hz_to_mel(sample_rate / 2)\n",
    "    \n",
    "    # Create mel points\n",
    "    mel_points = np.linspace(low_mel, high_mel, n_filters + 2)\n",
    "    hz_points = mel_to_hz(mel_points)\n",
    "    bin_points = np.floor(hz_points * N_fft / sample_rate).astype(int)\n",
    "    \n",
    "    # Create filter bank matrix\n",
    "    filters = np.zeros((n_filters, N_fft // 2))\n",
    "    \n",
    "    for i in range(1, n_filters + 1):\n",
    "        left = bin_points[i - 1]\n",
    "        center = bin_points[i]\n",
    "        right = bin_points[i + 1]\n",
    "        \n",
    "        # Left slope\n",
    "        for j in range(left, center):\n",
    "            if j < filters.shape[1]:\n",
    "                filters[i - 1, j] = (j - left) / (center - left)\n",
    "        \n",
    "        # Right slope\n",
    "        for j in range(center, right):\n",
    "            if j < filters.shape[1]:\n",
    "                filters[i - 1, j] = (right - j) / (right - center)\n",
    "    \n",
    "    return filters\n",
    "\n",
    "def apply_filterbanks(magnitudes, filters):\n",
    "    # Vectorized filterbank application\n",
    "    # Ensure magnitudes length matches filter bank width\n",
    "    mag_len = min(len(magnitudes), filters.shape[1])\n",
    "    magnitudes_truncated = magnitudes[:mag_len]\n",
    "    filters_truncated = filters[:, :mag_len]\n",
    "    \n",
    "    # Apply filters using matrix multiplication\n",
    "    energies = np.dot(filters_truncated, magnitudes_truncated)\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    energies = np.log(energies + 1e-10)\n",
    "    return energies\n",
    "\n",
    "def dct(signal):\n",
    "    # Vectorized DCT implementation\n",
    "    N = len(signal)\n",
    "    n = np.arange(N)\n",
    "    k = np.arange(N).reshape(-1, 1)\n",
    "    \n",
    "    # DCT matrix\n",
    "    dct_matrix = np.cos(np.pi * k * (2 * n + 1) / (2 * N))\n",
    "    result = np.dot(dct_matrix, signal)\n",
    "    return result\n",
    "\n",
    "def mfcc(samples, sample_rate, num_filters=26, num_coeffs=13):\n",
    "    print(\"üî∏ Calculating MFCC...\")\n",
    "    \n",
    "    # Pre-emphasis\n",
    "    emphasized = pre_emphasis(samples)\n",
    "    \n",
    "    # Windowing\n",
    "    frame_size = int(0.025 * sample_rate)\n",
    "    frame = emphasized[:frame_size]\n",
    "    hamming = hamming_window(len(frame))\n",
    "    windowed = frame * hamming\n",
    "    \n",
    "    # FFT\n",
    "    spectrum = fft(windowed)\n",
    "    \n",
    "    # Mel filterbank\n",
    "    filters = mel_filterbank(num_filters, len(spectrum) * 2, sample_rate)\n",
    "    \n",
    "    # Apply filterbanks\n",
    "    energies = apply_filterbanks(spectrum, filters)\n",
    "    \n",
    "    # DCT\n",
    "    cepstrals = dct(energies)\n",
    "    mfccs = cepstrals[:num_coeffs]\n",
    "    \n",
    "    print(f\"üîπ MFCC (13 coeff): {np.round(mfccs, 2).tolist()}\")\n",
    "    return mfccs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6196ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature Extraction ===\n",
    "def extract_features(file_path):\n",
    "    print(f\"\\nüì• Feature extraction from: {file_path}\")\n",
    "    samples, sample_rate = read_wav(file_path)\n",
    "    \n",
    "    zcr = zero_crossing_rate(samples)\n",
    "    en = energy(samples)\n",
    "    \n",
    "    # Use first 512 samples for FFT\n",
    "    fft_samples = samples[:512] if len(samples) >= 512 else samples\n",
    "    fft_mags = fft(fft_samples)\n",
    "    dom_freq = dominant_freq(fft_mags, sample_rate)\n",
    "    \n",
    "    mfcc_feats = mfcc(samples, sample_rate)\n",
    "    \n",
    "    return [zcr, en, dom_freq] + mfcc_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca77934",
   "metadata": {},
   "source": [
    "# 1.3 Load, encode label, and split dataset for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d427b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Dataset ===\n",
    "def load_dataset(folder_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    print(f\"üìÅ Loading dataset from: {folder_path}\")\n",
    "    \n",
    "    for label in os.listdir(folder_path):\n",
    "        label_folder = os.path.join(folder_path, label)\n",
    "        if not os.path.isdir(label_folder):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüóÇÔ∏è Label: {label}\")\n",
    "        for filename in os.listdir(label_folder):\n",
    "            if not filename.lower().endswith('.wav'):\n",
    "                continue  # Lewati .png dan .py\n",
    "                \n",
    "            path = os.path.join(label_folder, filename)\n",
    "            try:\n",
    "                features = extract_features(path)\n",
    "                data.append(features)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to process {path}: {e}\")\n",
    "    \n",
    "    return np.array(data), labels\n",
    "\n",
    "def encode_labels(labels):\n",
    "    unique_labels = list(set(labels))\n",
    "    label_to_int = {label: i for i, label in enumerate(unique_labels)}\n",
    "    print(f\"\\nüî¢ Label encoding: {label_to_int}\")\n",
    "    encoded_labels = np.array([label_to_int[label] for label in labels])\n",
    "    return encoded_labels, label_to_int\n",
    "\n",
    "def train_test_split(data, labels, ratio=0.8, shuffle=False):\n",
    "    n_train = int(len(data) * ratio)\n",
    "    \n",
    "    if shuffle:\n",
    "        # Shuffle data before splitting\n",
    "        indices = np.random.permutation(len(data))\n",
    "        train_indices = indices[:n_train]\n",
    "        test_indices = indices[n_train:]\n",
    "        print(f\"üîÑ Split data (shuffled): {n_train} train, {len(data) - n_train} test\")\n",
    "        return (data[train_indices], labels[train_indices],\n",
    "                data[test_indices], labels[test_indices])\n",
    "    else:\n",
    "        # Sequential split (like original)\n",
    "        print(f\"üîÑ Split data (sequential): {n_train} train, {len(data) - n_train} test\")\n",
    "        return (data[:n_train], labels[:n_train],\n",
    "                data[n_train:], labels[n_train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./data/sounds\"\n",
    "\n",
    "data, labels = load_dataset(dataset_path)\n",
    "labels_encoded, label_map = encode_labels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435eb612",
   "metadata": {},
   "source": [
    "# **2. KNN Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3473b",
   "metadata": {},
   "source": [
    "# 2.1 Define KNN model and evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54afc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === K-NN ===\n",
    "def euclidean_distance(a, b):\n",
    "    # Vectorized Euclidean distance\n",
    "    return np.sqrt(np.sum((a - b)**2))\n",
    "\n",
    "def predict_knn(train_data, train_labels, test_point, k=3):\n",
    "    # Vectorized distance calculation for all training points\n",
    "    distances = np.array([euclidean_distance(test_point, td) for td in train_data])\n",
    "    \n",
    "    # Get k nearest neighbors\n",
    "    k_indices = np.argsort(distances)[:k]\n",
    "    k_labels = train_labels[k_indices]\n",
    "    \n",
    "    # Return most common label\n",
    "    unique_labels, counts = np.unique(k_labels, return_counts=True)\n",
    "    return unique_labels[np.argmax(counts)]\n",
    "\n",
    "def evaluate(train_data, train_labels, test_data, test_labels):\n",
    "    print(\"\\nüß™ Evaluate model (k-NN)...\")\n",
    "    \n",
    "    predictions = []\n",
    "    for i, test_point in enumerate(test_data):\n",
    "        pred = predict_knn(train_data, train_labels, test_point)\n",
    "        predictions.append(pred)\n",
    "        actual = test_labels[i]\n",
    "        print(f\"   ‚ñ∏ Test {i+1}: Pred = {pred}, Label = {actual} --> {'‚úÖ' if pred == actual else '‚ùå'}\")\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    accuracy = np.mean(predictions == test_labels)\n",
    "    print(f\"\\nüéØ Accuracy: {accuracy*100:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Train and evaluate k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0771de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = train_test_split(data, labels_encoded, shuffle=False)\n",
    "\n",
    "evaluate(train_data, train_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab8afd6",
   "metadata": {},
   "source": [
    "# 2.3 Save model as json for streamlit use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4ab950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# === Save Model ===\n",
    "def save_model(train_data, train_labels, label_map, filename='siren_knn_model.json'):\n",
    "    model = {\n",
    "        'data': train_data.tolist(),\n",
    "        'labels': train_labels.tolist(),\n",
    "        'label_map': label_map\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(model, f)\n",
    "    print(f\"\\n‚úÖ Model saved as: {filename}\")\n",
    "\n",
    "save_model(train_data, train_labels, label_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
